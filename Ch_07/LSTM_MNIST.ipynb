{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T08:48:59.645323Z",
     "start_time": "2025-07-28T08:48:59.635479Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "from conda.exports import download\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "torch.manual_seed(125)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(125)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:48:59.660403Z",
     "start_time": "2025-07-28T08:48:59.657999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (1.0,))\n",
    "])"
   ],
   "id": "8325b6fd00c2d649",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:49:20.375261Z",
     "start_time": "2025-07-28T08:48:59.672742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "download_root = '../data/chap07/MNIST_DATASET'\n",
    "\n",
    "train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n",
    "valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)\n",
    "test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"
   ],
   "id": "96e28614f78d21e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:12<00:00, 806kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/chap07/MNIST_DATASET/MNIST/raw/train-images-idx3-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/chap07/MNIST_DATASET/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 920kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/chap07/MNIST_DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.59MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/chap07/MNIST_DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/chap07/MNIST_DATASET/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:49:20.396001Z",
     "start_time": "2025-07-28T08:49:20.390629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "valid_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)"
   ],
   "id": "725b313cc07a3390",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:49:20.416135Z",
     "start_time": "2025-07-28T08:49:20.414537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# images, labels = next(iter(train_loader))\n",
    "#\n",
    "# # Print the shape and other info\n",
    "# print(f\"Number of images in one batch: {images.shape[0]}\")\n",
    "# print(f\"Image tensor shape: {images.shape}\")\n",
    "# print(f\"Label tensor shape: {labels.shape}\")\n",
    "# print(f\"Data type of image tensor: {images.dtype}\")\n",
    "# print(f\"Data type of label tensor: {labels.dtype}\")\n",
    "# print(f\"Sample labels: {labels[:5]}\")"
   ],
   "id": "e99a082e36a5fc11",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:28:35.864988Z",
     "start_time": "2025-07-31T03:28:35.654354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "n_iters = 6000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)"
   ],
   "id": "c28819c0b5523e43",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m batch_size = \u001B[32m100\u001B[39m\n\u001B[32m      2\u001B[39m n_iters = \u001B[32m6000\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m num_epochs = n_iters / (\u001B[38;5;28mlen\u001B[39m(train_dataset) / batch_size)\n\u001B[32m      4\u001B[39m num_epochs = \u001B[38;5;28mint\u001B[39m(num_epochs)\n",
      "\u001B[31mNameError\u001B[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T03:28:35.882957Z",
     "start_time": "2025-07-28T08:49:20.436679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        hx, cx = hidden\n",
    "        x = x.view(-1, x.size(1))\n",
    "\n",
    "        # one for short term, one for input\n",
    "        gates = self.x2h(x) + self.h2h(hx)\n",
    "        # removes any dimensions of a tensor that have a size of 1\n",
    "        # parameter로 n 넣어주면 n차원인 차원을 없애줘, 차원을 줄여\n",
    "        # In neural network processing, data is often kept in batches, even if the batch size is just one. This line is a utility to flatten the tensor by removing any unnecessary dimensions, which can sometimes simplify subsequent calculations.\n",
    "        gates = gates.squeeze()\n",
    "        # 하나의 Linear layer로 계산 했던거 분리시켜\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = F.sigmoid(ingate)\n",
    "        forgetgate = F.sigmoid(forgetgate)\n",
    "        cellgate = F.tanh(cellgate)\n",
    "        outgate = F.sigmoid(outgate)\n",
    "\n",
    "        # torch.mul(): element-wise multiplication 적용\n",
    "        cy = torch.mul(cx, forgetgate) + torch.mul(ingate, cellgate)\n",
    "        hy = torch.mul(outgate, F.tanh(cy))\n",
    "        return (hy, cy)"
   ],
   "id": "2272251bcb00dcfb",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:49:20.452411Z",
     "start_time": "2025-07-28T08:49:20.448548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim  # 은닉층의 뉴런/유닛 갯수\n",
    "\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = LSTMCell(input_dim, hidden_dim, layer_dim)  # layer_dim looks total nonsense to me\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden 값과 cell 값을 초기화\n",
    "        # (은닉층의 계층 개수, 배치 크기, 은닉층의 뉴런 개수)\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda()\n",
    "        else:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda()\n",
    "        else:\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "\n",
    "        outs = []\n",
    "        # list slicing, [1번째 차원의 0번째 원소, 2번째 차원 전부다, 3번째 차원 전부다]\n",
    "        cn = c0[0, :, :]\n",
    "        hn = h0[0, :, :]\n",
    "\n",
    "        for seq in range(x.size(1)):  # input tensor x: (batch_size, sequence_length, input_features)\n",
    "            # In PyTorch, when you create a class that inherits from nn.Module and then call an instance of it like a function, you are automatically invoking its forward method.\n",
    "            hn, cn = self.lstm(x[:, seq, :], (hn, cn)) # 2d tensor [batch_size, actual_input], memories for( hidden state, cell state))\n",
    "            outs.append(hn)\n",
    "\n",
    "        out = outs[-1].squeeze()  # 최종 결과를 1차원 배열로 squeze (b/c fc로 넘길거 잖아)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ],
   "id": "e1351a2b81e305cf",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:49:20.464039Z",
     "start_time": "2025-07-28T08:49:20.461028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 128\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimzer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "id": "c83e066886d24f09",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:51:23.232760Z",
     "start_time": "2025-07-28T08:49:20.474300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_dim = 28\n",
    "loss_list = []\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.view(-1, seq_dim, input_dim).cuda()\n",
    "            labels = labels.cuda()\n",
    "        else:\n",
    "            images = images.view(-1, seq_dim, input_dim)\n",
    "            labels = labels\n",
    "\n",
    "        optimzer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss.cuda()\n",
    "\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        loss_list.append(loss.item())\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.view(-1, seq_dim, input_dim).cuda()\n",
    "                else:\n",
    "                    images = images.view(-1, seq_dim, input_dim)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ],
   "id": "a6bd792f1842d05c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.237457513809204. Accuracy: 21.420000076293945\n",
      "Iteration: 1000. Loss: 0.9100692868232727. Accuracy: 74.06999969482422\n",
      "Iteration: 1500. Loss: 0.4141859710216522. Accuracy: 87.98999786376953\n",
      "Iteration: 2000. Loss: 0.31473594903945923. Accuracy: 91.62999725341797\n",
      "Iteration: 2500. Loss: 0.09829992055892944. Accuracy: 93.63999938964844\n",
      "Iteration: 3000. Loss: 0.07892906665802002. Accuracy: 96.4800033569336\n",
      "Iteration: 3500. Loss: 0.09954178333282471. Accuracy: 96.30999755859375\n",
      "Iteration: 4000. Loss: 0.013582933694124222. Accuracy: 97.12000274658203\n",
      "Iteration: 4500. Loss: 0.05706917867064476. Accuracy: 97.12999725341797\n",
      "Iteration: 5000. Loss: 0.08537448197603226. Accuracy: 97.27999877929688\n",
      "Iteration: 5500. Loss: 0.0972917452454567. Accuracy: 97.05000305175781\n",
      "Iteration: 6000. Loss: 0.026007307693362236. Accuracy: 97.81999969482422\n",
      "Iteration: 6500. Loss: 0.021794160827994347. Accuracy: 97.62999725341797\n",
      "Iteration: 7000. Loss: 0.06000397726893425. Accuracy: 97.86000061035156\n",
      "Iteration: 7500. Loss: 0.031310077756643295. Accuracy: 97.73999786376953\n",
      "Iteration: 8000. Loss: 0.05826945975422859. Accuracy: 97.97000122070312\n",
      "Iteration: 8500. Loss: 0.010482029989361763. Accuracy: 98.2300033569336\n",
      "Iteration: 9000. Loss: 0.040576327592134476. Accuracy: 97.80000305175781\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T08:51:24.605615Z",
     "start_time": "2025-07-28T08:51:23.245583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, val_iter):\n",
    "    corrects, total, total_loss = 0, 0, 0\n",
    "    model.eval()\n",
    "    for images, labels in val_iter:\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.view(-1, seq_dim, input_dim).cuda()\n",
    "        else:\n",
    "            images = images.view(-1, seq_dim, input_dim).to(device)\n",
    "\n",
    "        logit = model(images).to(device)\n",
    "        loss = F.cross_entropy(logit, labels, reduction=\"sum\")\n",
    "        _, predicted = torch.max(logit.data, 1)\n",
    "        total += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        corrects += (predicted == labels).sum()\n",
    "\n",
    "    avg_loss = total_loss / len(val_iter.dataset)\n",
    "    avg_accuracy = corrects / total\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(\"Test Loss: %5.2f | Test Accuracy: %5.2f\" % (test_loss, test_acc))"
   ],
   "id": "bfdc4fbdd4dfda1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.06 | Test Accuracy:  0.98\n"
     ]
    }
   ],
   "execution_count": 161
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
